<!DOCTIPE html>
<html lang="ja">

<<head>
  <meta charset="utf-8" >
  <title>E資格挑戦記　〜課題挑戦の軌跡〜</title>
  <meta name="description" content="E資格受験のための勉強記録を記します。">
  <meta name="keywords" content="E資格">
  <link rel="stylesheet" href="style.css" type="text/css" media ="screen">
</head>


<body>
<!-- #wrapprer ここから-->

<div id="wrapper">
    <header>
      <h1>
        <img src="images/title2.png" alt="E資格挑戦記　〜課題挑戦の軌跡〜">
      </h1>
    </header>

    <div id="left">  </div>

    <div id="right">
      <div class="textarea">
        <h1>テーマ：機械学習 </h1>
        
        <h2>Ⅰ.線形回帰モデル</h2>

        <h3>1.要点</h3>
        <h4>回帰とは、既知のデータから予測を行うこと。線形回帰では1次関数を用いて予測を行う。<br>
        線形回帰モデルは機械学習モデルの1つであり、入力データとパラメータを線形結合させることで予測値を出力する。<br>
        学習の流れ方法は、MSE（最小二乗誤差）の微分が0になるパラメータを算出する。<br>
        <br>
        </h4>
        
        <h3>2.実装演習</h3>
        <h4>課題：部屋数が4で犯罪率が0.３の物件はいくらになるか？<br>
        結果：4240ドル</h4>
 
        
        <h2>II.非線形回帰モデル
        </h2>
        <h3>1.要点</h3>
        <h4>
        線型で表現できることは限られている。非線形構造をとらえる為の関数が必要。<br>
        非線形を表現する方法として、基底関数とパラメータベクトルとの線形結用をさせる手法がある。<br>
        未知のパラメータは最小二乗法や最尤法により推定<br>
        関数は多項式やガウス基底関数やスプライン関数などがある。<br>
        計算は線形回帰モデルの手法と同様に、目的関数との差分より計算されるMSEの微分が０となるパラメータを算出する。<br>
        学習の進行具合として未学習(Underfitting)と過学習(Overfitting)がある。<br>
        未学習(Underfitting)は、訓練誤差、検証誤差共に大きい状態で学習が進められていない状態。<br>
        過学習(Overfitting)は、訓練データに最適化され過ぎることで検証誤差が大きくなる状態。<br>
        過学習を防ぐために、基底関数の数を制限する、正則化により複雑な関数にならないように制限する。<br>
        正則化Ridge推定量ではパラメータをできるだけ小さくするのに対して、Lasso推定量では正確に0にする。<br>
        汎化性能が高いかは交差検証で判断。<br>
        交差検証はn分割したデータの1/nを検証データ、残りの（n-1)/nのデータを訓練に用いる。<br>
        順番に検証データを変えてn回検証を行い、その平均をパラメータとすることで、モデルの汎化性能を評価できる<br>
        ホールドアウトは分割するだけ。データ量が多い時には使用してもいいが、データ量が少ない時には交差検証の方が良い。<br>
        ハイパーパラメーターはグリッドサーチが一般的。グリッドサーチは、絨毯爆撃的に検証しする方法。<br>
        </h4>
        <h4>非線形回帰とCVを実装<br>
        結果：非線形回帰で相関係数は0.87まで改善</h4>
        
   
        
        <h2>III.ロジスティック回帰モデル</h2>
        <h3>1.要点</h3>
        <h4>クラス分類に用いられる。<br>
        入力値とパラメータの線形結合の値から分類をする必要があるために、<br> 
        連続値を0か1に分類可能な関数であるシグモイド関数を用いる。<br> 
        シグモイド関数は微分が簡単だから尤度関数の微分の際に計算の簡略化が可能。
        <br> 
        
        尤度とはもっともらしさ。<br> 
        ロジスティック回帰モデルの構築のためには、尤度関数を最大化するパラメータを探索する。<br> 
        つまりは、パラメータを調整して、もっともらしいモデルを作る。<br> 
        
        パラメータ探索には勾配降下方を用いる必要がある。<br> 
        なぜならば、対数尤度関数をパラメータで微分し、0になるパラメータを解析的に数字を得ることができないためである。<br> 
        
        学習率ηはハイパーパラメータで学習の速度や学習結果に大きく影響する。<br> 
        勾配降下法はすべてのデータに対して適用するために、データ量が多い場合には学習に時間がかかる。<br> 
        そこで、確率的勾配降下法により、一部のデータからランダムに学習することで計算コストを下げることができる。<br> 
        
        回帰の性能評価には4つの視点で評価される。<br> 
        （真陽性、偽陽性、偽陰性、真陰性）<br> 
        
        評価指標としてリコールやプレシジョンを使う<br> 
        リコール：陽性を感度高く検出する（陽性を見逃したくない時に用いる）。<br> 
        プレシジョン：適合率を高め、見逃しは考えない（陽性の確実性が必要な場合に用いる。）<br> 
      </h4>
        <h3>2.実装演習/追加演習</h3>
        <h4>タイタニック乗客データを利用し、ロジスティック回帰モデルを作成<br>
        特徴量を抽出してみる<br>
        課題：30歳男性の乗客は生き残れるか？<br>
        結果：生還率は100%となった。正しいアプローチではない可能性あり。<br>
      </h4>
        
        <h2>IV.主成分分析</h2>
        <h3>1.要点</h3>
        <h4>情報量をなるべく維持したまま次元を削減する事でデータの情報量を削減する手法。<br>
        ノルムが１となる制約を入れる事で、解が無数に発生するのを防ぐ。<br>
        計算方法はラグランジュ関数を最大にするベクトルを探索する。<br>
        
        分散の最大化する軸を第1主成分、主成分に直行し且つ分散の最大となる軸を第２主成分という。<br>
        寄与率によりその成分の有する情報量を算出できる。<br>
      </h4>
        <h3>2.実装演習/追加演習</h3>
        <h4>
        ・設定
        乳がん検査データを利用し、ロジスティック回帰モデルを作成<br>
        主成分を利用して二次元空間上に次元圧縮する。<br>
        
        課題：<br>
        32次元を２次元に圧縮した際に、うまく判別できるかを確認。<br>
        結果：<br>
        32次元でのロジスティック回帰では92.3%の正答率であったが、<br>
        2次元に圧縮しても94.0%（少し良化しているのが気になるが理由は不明。）と、情報のロスは発生していない結果となった。<br>
      </h4>
        
        <h2>V.アルゴリズム</h2>
        <h3>1.要点</h3>
        <h4>[1]k近傍法(kNN)<br>
        対象となる点の近傍のk個のうち、多く属するクラスとして識別する方法。<br>
        k（識別個数）を大きくすると識別境界が滑らかになる。<br>
        
        [2]k-平均砲（k-means)<br>
        教師なし学習のクラスタリング手法。任意に与えたクラスにデータを分類する。<br>
        セントロイドの位置をランダムに与え、距離の近いクラスにデータを分類したのちに、<br>
        セントロイドの位置をクラスに属するデータとの距離を小さくする方向に修正する。<br>
        クラス分類と位置修正を繰り返し行うことでクラスタリングする。<br>
        
        セントロイドの位置の違いによりクラスタリング結果がばらつくことから、<br>
        距離解析から位置を決めるk-means++という手法が考案されている。<br>
      </h4>
        <h3>2.実装演習/追加演習</h3>
        <h4>[1]k近傍法(kNN)
        人口データを分類<br>
        課題：人口データと分類結果をプロット<br>
 
        [2]k-平均法（k-means)<br>
        人口データを分類<br>
        課題：人口データと分類結果をプロット<br>
      </h4>
        <h2>VI.サポートベクターマシン</h2>
        <h3>1.要点</h3>
        <h4>線形分離による分類を基本原理としており、<br>
          線形識別境界からもっとも近いデータとの距離（マージン）を最大化することで分類する。<br>
          マージンを決めるデータ点が境界を決めるためデータ数が少なくても分離できる。<br>
        KKT条件にてラグランジュ未定乗数法を用いて解を得る。<br>
        ソフトマージンSVMなど、境界の決定条件を緩める手法も考案されている。<br>
        
        非線形の分離境界を得る為の手法としてカーネル法がある。<br>
        カーネル法は、カーネル関数を用いて、高次元に写像することで線形分離可能とする手法。<br>
        カーネルトリックにより計算負荷を大幅に低減している為、高次元データでも計算負荷が小さい。<br>
        
        2.実装演習<br>
        Irisデータセットの分類<br>
      </h4>
        


      </div>

    </div>

    <div id="clearfix"></div>
</div>
<!-- #wrapprer ここまで-->

<footer></footer>
</body>
